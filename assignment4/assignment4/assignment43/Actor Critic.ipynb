{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"Actor Critic.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"M8268S3KDK_N"},"source":["# Actor Critic Algorithms\n","In the DQN algorithm, we learned a Q-function by minimizing Bellman errors for an implicit policy that always took the action that maximized the Q-function. However, this scheme requires a discrete action spaces to allow for us to easily compute the optimal action at each state, unlike generic policy gradient algorithms that also worked with continuous action spaces.\n","\n","In this section, we will explore actor-critic algorithms which maintain an explicit policy (actor) like the policy gradient algorithms, learns a Q-function (critic) capturing the values of the _current policy_, and uses this learned Q-function to update the policy. Using a learned critic can provide much lower variance updates for the policy compared to using Monte-Carlo retun estimates, and also allows us to reuse our data by training the actor and critic on _off-policy_ data for more sample efficiency. We can thus take many more policy updates with an actor critic algorithm using our learned critic, instead of needing to wait and gather fresh samples every time."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VVz4D48ZW16h","executionInfo":{"status":"ok","timestamp":1616692183017,"user_tz":-480,"elapsed":907,"user":{"displayName":"Anthony Ng","photoUrl":"","userId":"16843483630260605487"}},"outputId":"6e51eb53-01ec-4ca4-f2d5-2159e8f36389"},"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","DRIVE_PATH = '/content/drive/My\\ Drive/282'"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"egr_lI3bXQ0x","executionInfo":{"status":"ok","timestamp":1616692312749,"user_tz":-480,"elapsed":121818,"user":{"displayName":"Anthony Ng","photoUrl":"","userId":"16843483630260605487"}},"outputId":"6b4e4c88-8bbc-4c04-a8d4-efb6d973098b"},"source":["DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n","if not os.path.exists(DRIVE_PYTHON_PATH):\n","  %mkdir $DRIVE_PATH\n","\n","## the space in `My Drive` causes some issues,\n","## make a symlink to avoid this\n","SYM_PATH = '/content/282'\n","if not os.path.exists(SYM_PATH):\n","  !ln -s $DRIVE_PATH $SYM_PATH\n","!apt update \n","!apt install -y --no-install-recommends \\\n","        build-essential \\\n","        curl \\\n","        git \\\n","        gnupg2 \\\n","        make \\\n","        cmake \\\n","        ffmpeg \\\n","        swig \\\n","        libz-dev \\\n","        unzip \\\n","        zlib1g-dev \\\n","        libglfw3 \\\n","        libglfw3-dev \\\n","        libxrandr2 \\\n","        libxinerama-dev \\\n","        libxi6 \\\n","        libxcursor-dev \\\n","        libgl1-mesa-dev \\\n","        libgl1-mesa-glx \\\n","        libglew-dev \\\n","        libosmesa6-dev \\\n","        lsb-release \\\n","        ack-grep \\\n","        patchelf \\\n","        wget \\\n","        xpra \\\n","        xserver-xorg-dev \\\n","        xvfb \\\n","        python-opengl \\\n","        ffmpeg > /dev/null 2>&1\n","MJC_PATH = '{}/mujoco'.format(SYM_PATH)\n","if not os.path.exists(MJC_PATH):\n","  %mkdir $MJC_PATH\n","%cd $MJC_PATH\n","if not os.path.exists(os.path.join(MJC_PATH, 'mujoco200')):\n","  !wget -q https://www.roboti.us/download/mujoco200_linux.zip\n","  !unzip -q mujoco200_linux.zip\n","  %mv mujoco200_linux mujoco200\n","  %rm mujoco200_linux.zip\n","import os\n","\n","os.environ['LD_LIBRARY_PATH'] += ':{}/mujoco200/bin'.format(MJC_PATH)\n","os.environ['MUJOCO_PY_MUJOCO_PATH'] = '{}/mujoco200'.format(MJC_PATH)\n","os.environ['MUJOCO_PY_MJKEY_PATH'] = '{}/mjkey.txt'.format(MJC_PATH)\n","\n","## installation on colab does not find *.so files\n","## in LD_LIBRARY_PATH, copy over manually instead\n","!cp $MJC_PATH/mujoco200/bin/*.so /usr/lib/x86_64-linux-gnu/\n","%cd $MJC_PATH\n","if not os.path.exists('mujoco-py'):\n","  !git clone https://github.com/openai/mujoco-py.git\n","%cd mujoco-py\n","%pip install -e .\n","\n","## cythonize at the first import\n","import mujoco_py\n","%cd $SYM_PATH\n","\n","%cd assignment4\n","%pip install -r requirements.txt"],"execution_count":5,"outputs":[{"output_type":"stream","text":["\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [1 InRelease 14.2 kB/88.7\u001b[0m\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting f\u001b[0m\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n","Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Get:10 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,045 kB]\n","Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Get:13 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [348 kB]\n","Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,402 kB]\n","Hit:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Get:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n","Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,748 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,170 kB]\n","Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [894 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,475 kB]\n","Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [378 kB]\n","Get:24 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [49.4 kB]\n","Fetched 11.8 MB in 4s (3,322 kB/s)\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","59 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","/content/drive/My Drive/282/mujoco\n","/content/drive/My Drive/282/mujoco\n","/content/drive/My Drive/282/mujoco/mujoco-py\n","Obtaining file:///content/drive/My%20Drive/282/mujoco/mujoco-py\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","\u001b[33m  WARNING: Missing build requirements in pyproject.toml for file:///content/drive/My%20Drive/282/mujoco/mujoco-py.\u001b[0m\n","\u001b[33m  WARNING: The project does not specify a build backend, and pip cannot fall back to setuptools without 'wheel'.\u001b[0m\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Collecting fasteners~=0.15\n","  Using cached https://files.pythonhosted.org/packages/78/20/c862d765287e9e8b29f826749ebae8775bdca50b2cb2ca079346d5fbfd76/fasteners-0.16-py2.py3-none-any.whl\n","Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.7/dist-packages (from mujoco-py==2.0.2.13) (0.29.22)\n","Requirement already satisfied: imageio>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from mujoco-py==2.0.2.13) (2.4.1)\n","Collecting glfw>=1.4.0\n","  Using cached https://files.pythonhosted.org/packages/13/d7/79c091c877493de7f8286ed62c77bf0f2c51105656073846b2326021b524/glfw-2.1.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from mujoco-py==2.0.2.13) (1.19.5)\n","Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.7/dist-packages (from mujoco-py==2.0.2.13) (1.14.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fasteners~=0.15->mujoco-py==2.0.2.13) (1.15.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio>=2.1.2->mujoco-py==2.0.2.13) (7.0.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.10->mujoco-py==2.0.2.13) (2.20)\n","Installing collected packages: fasteners, glfw, mujoco-py\n","  Running setup.py develop for mujoco-py\n","Successfully installed fasteners-0.16 glfw-2.1.0 mujoco-py\n","/content/drive/My Drive/282\n","/content/drive/My Drive/282/assignment4\n","Collecting gym[atari]==0.17.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/99/7cc3e510678119cdac91f33fb9235b98448f09a6bdf0cafea2b108d9ce51/gym-0.17.2.tar.gz (1.6MB)\n","\u001b[K     |████████████████████████████████| 1.6MB 4.3MB/s \n","\u001b[?25hRequirement already satisfied: mujoco-py==2.0.2.13 in /content/drive/My Drive/282/mujoco/mujoco-py (from -r requirements.txt (line 2)) (2.0.2.13)\n","Collecting tensorboard==2.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/1b/6a420d7e6ba431cf3d51b2a5bfa06a958c4141e3189385963dc7f6fbffb6/tensorboard-2.3.0-py3-none-any.whl (6.8MB)\n","\u001b[K     |████████████████████████████████| 6.8MB 17.0MB/s \n","\u001b[?25hCollecting tensorboardX==1.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/12/dcaf67e1312475b26db9e45e7bb6f32b540671a9ee120b3a72d9e09bc517/tensorboardX-1.8-py2.py3-none-any.whl (216kB)\n","\u001b[K     |████████████████████████████████| 225kB 31.5MB/s \n","\u001b[?25hCollecting matplotlib==2.2.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/1d/e6d9af0b5045597869537391f1036ab841c613c3f3e40f16bbc1d75450ee/matplotlib-2.2.2-cp37-cp37m-manylinux1_x86_64.whl (12.6MB)\n","\u001b[K     |████████████████████████████████| 12.6MB 39.3MB/s \n","\u001b[?25hCollecting ipython==6.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/7f/91d50f28af3e3a24342561983a7857e399ce24093876e6970b986a0b6677/ipython-6.4.0-py3-none-any.whl (750kB)\n","\u001b[K     |████████████████████████████████| 757kB 31.7MB/s \n","\u001b[?25hCollecting moviepy==1.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/32/a93f4af8b88985304a748ca0a66a64eb9fac53d0a9355ec33e713c4a3bf5/moviepy-1.0.0.tar.gz (398kB)\n","\u001b[K     |████████████████████████████████| 399kB 52.2MB/s \n","\u001b[?25hCollecting pyvirtualdisplay==1.3.2\n","  Downloading https://files.pythonhosted.org/packages/d0/8a/643043cc70791367bee2d19eb20e00ed1a246ac48e5dbe57bbbcc8be40a9/PyVirtualDisplay-1.3.2-py2.py3-none-any.whl\n","Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (1.8.0+cu101)\n","Collecting opencv-python==4.4.0.42\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/e3/7ed67a8f3116113a364671fb4142c446dd804c63f3d9df5c11168a1e4dbb/opencv_python-4.4.0.42-cp37-cp37m-manylinux2014_x86_64.whl (49.4MB)\n","\u001b[K     |████████████████████████████████| 49.4MB 149kB/s \n","\u001b[?25hCollecting ipdb==0.13.3\n","  Downloading https://files.pythonhosted.org/packages/c1/4c/c2552dc5c2f3a4657ae84c1a91e3c7d4f2b7df88a38d6d282e48d050ad58/ipdb-0.13.3.tar.gz\n","Collecting box2d-py\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n","\u001b[K     |████████████████████████████████| 450kB 40.4MB/s \n","\u001b[?25hCollecting numpy==1.20.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/6c/322f6aa128179d0ea45a543a4e29a74da2317117109899cfd56d09bf3de0/numpy-1.20.0-cp37-cp37m-manylinux2010_x86_64.whl (15.3MB)\n","\u001b[K     |████████████████████████████████| 15.3MB 515kB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[atari]==0.17.2->-r requirements.txt (line 1)) (1.4.1)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]==0.17.2->-r requirements.txt (line 1)) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]==0.17.2->-r requirements.txt (line 1)) (1.3.0)\n","Requirement already satisfied: atari_py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]==0.17.2->-r requirements.txt (line 1)) (0.2.6)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari]==0.17.2->-r requirements.txt (line 1)) (7.0.0)\n","Requirement already satisfied: glfw>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from mujoco-py==2.0.2.13->-r requirements.txt (line 2)) (2.1.0)\n","Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.7/dist-packages (from mujoco-py==2.0.2.13->-r requirements.txt (line 2)) (0.29.22)\n","Requirement already satisfied: imageio>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from mujoco-py==2.0.2.13->-r requirements.txt (line 2)) (2.4.1)\n","Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.7/dist-packages (from mujoco-py==2.0.2.13->-r requirements.txt (line 2)) (1.14.5)\n","Requirement already satisfied: fasteners~=0.15 in /usr/local/lib/python3.7/dist-packages (from mujoco-py==2.0.2.13->-r requirements.txt (line 2)) (0.16)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 3)) (1.8.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 3)) (1.27.1)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 3)) (3.12.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 3)) (0.4.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 3)) (2.23.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 3)) (3.3.4)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 3)) (0.10.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 3)) (54.1.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 3)) (1.0.1)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 3)) (0.36.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 3)) (1.15.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.3.0->-r requirements.txt (line 3)) (1.32.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.2.2->-r requirements.txt (line 5)) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.2.2->-r requirements.txt (line 5)) (1.3.1)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.2.2->-r requirements.txt (line 5)) (2018.9)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.2.2->-r requirements.txt (line 5)) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.2.2->-r requirements.txt (line 5)) (2.8.1)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython==6.4.0->-r requirements.txt (line 6)) (0.8.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython==6.4.0->-r requirements.txt (line 6)) (4.4.2)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython==6.4.0->-r requirements.txt (line 6)) (5.0.5)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython==6.4.0->-r requirements.txt (line 6)) (0.2.0)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython==6.4.0->-r requirements.txt (line 6)) (4.8.0)\n","Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.7/dist-packages (from ipython==6.4.0->-r requirements.txt (line 6)) (0.18.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython==6.4.0->-r requirements.txt (line 6)) (0.7.5)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython==6.4.0->-r requirements.txt (line 6)) (2.6.1)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.15 in /usr/local/lib/python3.7/dist-packages (from ipython==6.4.0->-r requirements.txt (line 6)) (1.0.18)\n","Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.7/dist-packages (from moviepy==1.0.0->-r requirements.txt (line 7)) (4.41.1)\n","Collecting proglog<=1.0.0\n","  Downloading https://files.pythonhosted.org/packages/fe/ab/4cb19b578e1364c0b2d6efd6521a8b4b4e5c4ae6528041d31a2a951dd991/proglog-0.1.9.tar.gz\n","Collecting imageio_ffmpeg>=0.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/0f/4b49476d185a273163fa648eaf1e7d4190661d1bbf37ec2975b84df9de02/imageio_ffmpeg-0.4.3-py3-none-manylinux2010_x86_64.whl (26.9MB)\n","\u001b[K     |████████████████████████████████| 26.9MB 169kB/s \n","\u001b[?25hCollecting EasyProcess\n","  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->-r requirements.txt (line 9)) (3.7.4.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]==0.17.2->-r requirements.txt (line 1)) (0.16.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.10->mujoco-py==2.0.2.13->-r requirements.txt (line 2)) (2.20)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.3.0->-r requirements.txt (line 3)) (4.2.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.3.0->-r requirements.txt (line 3)) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.3.0->-r requirements.txt (line 3)) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.3.0->-r requirements.txt (line 3)) (1.3.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.3.0->-r requirements.txt (line 3)) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.3.0->-r requirements.txt (line 3)) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.3.0->-r requirements.txt (line 3)) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard==2.3.0->-r requirements.txt (line 3)) (3.0.4)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard==2.3.0->-r requirements.txt (line 3)) (3.7.2)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython==6.4.0->-r requirements.txt (line 6)) (0.2.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython==6.4.0->-r requirements.txt (line 6)) (0.7.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython==6.4.0->-r requirements.txt (line 6)) (0.8.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.15->ipython==6.4.0->-r requirements.txt (line 6)) (0.2.5)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard==2.3.0->-r requirements.txt (line 3)) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.3.0->-r requirements.txt (line 3)) (3.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard==2.3.0->-r requirements.txt (line 3)) (3.4.1)\n","Building wheels for collected packages: gym, moviepy, ipdb, proglog\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.17.2-cp37-none-any.whl size=1650891 sha256=22e4be33f8af649716416d4b0977c5d6e75a7f104baba94995a48d475ba44cb8\n","  Stored in directory: /root/.cache/pip/wheels/87/e0/91/f56e44e8062f8cd549673da49f59e1d4fe8b17398119b1d221\n","  Building wheel for moviepy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for moviepy: filename=moviepy-1.0.0-cp37-none-any.whl size=131366 sha256=44586821824ebb8e6abb769f699c6662d850aaa1bf36efacd251254a1f1c435c\n","  Stored in directory: /root/.cache/pip/wheels/52/e2/4c/f594a5945bc98e052ef248b46a0f1f7ea838b0b2a5f8895651\n","  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ipdb: filename=ipdb-0.13.3-cp37-none-any.whl size=10848 sha256=81fef500dd2678c46421eefb2e161a71e53bcbd25f8cf6a9363229ce565fb724\n","  Stored in directory: /root/.cache/pip/wheels/75/00/30/4169bcc3643f0cf946dcf37af1b71364b390c4df91da02b03c\n","  Building wheel for proglog (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for proglog: filename=proglog-0.1.9-cp37-none-any.whl size=6148 sha256=3dd87fafb193e20aea9ec0a303d1d2384c80ae64ff9e97875dfa62429b3f2991\n","  Stored in directory: /root/.cache/pip/wheels/65/56/60/1d0306a8d90b188af393c1812ddb502a8821b70917f82dcc00\n","Successfully built gym moviepy ipdb proglog\n","\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.20.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorboard~=2.4, but you'll have tensorboard 2.3.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: plotnine 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 2.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: moviepy 1.0.0 has requirement imageio<3.0,>=2.5; python_version >= \"3.4\", but you'll have imageio 2.4.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 2.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 6.4.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: numpy, opencv-python, gym, tensorboard, tensorboardX, matplotlib, ipython, proglog, imageio-ffmpeg, moviepy, EasyProcess, pyvirtualdisplay, ipdb, box2d-py\n","  Found existing installation: numpy 1.19.5\n","    Uninstalling numpy-1.19.5:\n","      Successfully uninstalled numpy-1.19.5\n","  Found existing installation: opencv-python 4.1.2.30\n","    Uninstalling opencv-python-4.1.2.30:\n","      Successfully uninstalled opencv-python-4.1.2.30\n","  Found existing installation: gym 0.17.3\n","    Uninstalling gym-0.17.3:\n","      Successfully uninstalled gym-0.17.3\n","  Found existing installation: tensorboard 2.4.1\n","    Uninstalling tensorboard-2.4.1:\n","      Successfully uninstalled tensorboard-2.4.1\n","  Found existing installation: matplotlib 3.2.2\n","    Uninstalling matplotlib-3.2.2:\n","      Successfully uninstalled matplotlib-3.2.2\n","  Found existing installation: ipython 5.5.0\n","    Uninstalling ipython-5.5.0:\n","      Successfully uninstalled ipython-5.5.0\n","  Found existing installation: moviepy 0.2.3.5\n","    Uninstalling moviepy-0.2.3.5:\n","      Successfully uninstalled moviepy-0.2.3.5\n","Successfully installed EasyProcess-0.3 box2d-py-2.3.8 gym-0.17.2 imageio-ffmpeg-0.4.3 ipdb-0.13.3 ipython-6.4.0 matplotlib-2.2.2 moviepy-1.0.0 numpy-1.20.0 opencv-python-4.4.0.42 proglog-0.1.9 pyvirtualdisplay-1.3.2 tensorboard-2.3.0 tensorboardX-1.8\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["IPython","matplotlib","mpl_toolkits","numpy"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"0X1motBHDK_P","executionInfo":{"status":"ok","timestamp":1616692321886,"user_tz":-480,"elapsed":9121,"user":{"displayName":"Anthony Ng","photoUrl":"","userId":"16843483630260605487"}}},"source":["# As usual, a bit of setup\n","import os\n","import shutil\n","import time\n","import numpy as np\n","import torch\n","\n","import deeprl.infrastructure.pytorch_util as ptu\n","from deeprl.infrastructure.rl_trainer import RL_Trainer\n","from deeprl.infrastructure.trainers import AC_Trainer\n","\n","from deeprl.policies.MLP_policy import MLPPolicyAC\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","def rel_error(x, y):\n","    \"\"\" returns relative error \"\"\"\n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n","\n","def remove_folder(path):\n","    # check if folder exists\n","    if os.path.exists(path): \n","        print(\"Clearing old results at {}\".format(path))\n","        # remove if exists\n","        shutil.rmtree(path)\n","    else:\n","        print(\"Folder {} does not exist yet. No old results to delete\".format(path))"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"4n-2Em8RDK_R","executionInfo":{"status":"ok","timestamp":1616692321886,"user_tz":-480,"elapsed":9117,"user":{"displayName":"Anthony Ng","photoUrl":"","userId":"16843483630260605487"}}},"source":["ac_base_args_dict = dict(\n","    env_name = 'Hopper-v2', #@param ['Ant-v2', 'Humanoid-v2', 'Walker2d-v2', 'HalfCheetah-v2', 'Hopper-v2']\n","    exp_name = 'test_ac', #@param\n","    save_params = False, #@param {type: \"boolean\"}\n","    \n","    ## PDF will tell you how to set ep_len\n","    ## and discount for each environment\n","    ep_len = 200, #@param {type: \"integer\"}\n","    discount = 0.99, #@param {type: \"number\"}\n","\n","    # Training\n","    num_agent_train_steps_per_iter = 1000, #@param {type: \"integer\"})\n","    n_iter = 100, #@param {type: \"integer\"})\n","\n","    # batches & buffers\n","    batch_size = 1000, #@param {type: \"integer\"})\n","    eval_batch_size = 1000, #@param {type: \"integer\"}\n","    train_batch_size = 256, #@param {type: \"integer\"}\n","    max_replay_buffer_size = 1000000, #@param {type: \"integer\"}\n","\n","    #@markdown actor network\n","    n_layers = 2, #@param {type: \"integer\"}\n","    size = 256, #@param {type: \"integer\"}\n","    entropy_weight=0, #@param {type: \"number\"}\n","    learning_rate = 3e-4, #@param {type: \"number\"}\n","    \n","    # critic network\n","    critic_n_layers = 2, #@param {type: \"integer\"}\n","    critic_size = 256, #@param {type: \"integer\"}\n","    target_update_rate = 5e-3,\n","\n","    #@markdown logging\n","    video_log_freq = -1, #@param {type: \"integer\"}\n","    scalar_log_freq = 1, #@param {type: \"integer\"}\n","\n","    #@markdown gpu & run-time settings\n","    no_gpu = False, #@param {type: \"boolean\"}\n","    which_gpu = 0, #@param {type: \"integer\"}\n","    seed = 2, #@param {type: \"integer\"}\n","    logdir = 'test',\n",")"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P7WQ2dYrDK_R"},"source":["First fill out the target value calculation in the compute_target_value method of <code>critics/bootstrapped_continuous_critic.py</code>. Compared to the DQN critic, the key difference is that we are now estimating the value of the current policy, instead of the optimal policy as in DQN or Q-learning.\n","\n","To train our critic to evaluate the current policy $\\pi$, we simply sample actions from the current policy in our target value. For each sample $(s,a,s')$, our loss will be\n","$$L(Q_{\\theta}(s, a), r(s,a) + \\gamma \\mathbb{E}_{a'\\sim \\pi(s')}[Q_{\\bar \\theta} (s', a')]),$$\n","where $L$ is our loss function (for example squared error or the smooth L1 loss).\n","In this assignment, we will simply sample a single action from the policy to estimate the target value."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yJ5xo3loDK_S","executionInfo":{"status":"ok","timestamp":1616695383789,"user_tz":-480,"elapsed":1159,"user":{"displayName":"Anthony Ng","photoUrl":"","userId":"16843483630260605487"}},"outputId":"4ff9d954-4d98-4436-aaed-8dc3f6bea16f"},"source":["# Test bellman error for policy evaluation\n","ac_dim = 3\n","ob_dim = 11\n","N = 5\n","\n","np.random.seed(0)\n","obs = np.random.normal(size=(N, ob_dim))\n","acts = np.random.choice(ac_dim, size=(N,))\n","next_obs = np.random.normal(size=(N, ob_dim))\n","rewards = np.random.normal(size=N)\n","terminals = np.zeros(N)\n","terminals[0] = 1\n","\n","ac_args = dict(ac_base_args_dict)\n","\n","env_str = 'Hopper'\n","ac_args['env_name'] = '{}-v2'.format(env_str)\n","ac_args['entropy_weight'] = 0.1\n","actrainer = AC_Trainer(ac_args)\n","critic = actrainer.rl_trainer.agent.critic\n","\n","class DummyDist:\n","    def sample(self):\n","        return ptu.from_numpy(1 + np.zeros(shape=(N, ac_dim)))\n","\n","def dummy_actor(next_obs):\n","    return DummyDist()\n","\n","# assumes you call actor(next_obs) to get the distribution, then call distribution.sample()\n","target_vals = critic.compute_target_value(ptu.from_numpy(next_obs), \n","                                          ptu.from_numpy(rewards), \n","                                          ptu.from_numpy(terminals), \n","                                          dummy_actor)\n","target_vals = ptu.to_numpy(target_vals)\n","expected_targets = np.array([-0.9167948, -0.11123351, -0.36787638, -2.1131861,  -0.13868617])\n","\n","target_error = rel_error(target_vals, expected_targets)\n","print(\"Target value error\", target_error, \"should be on the order of 1e-6 or lower\")"],"execution_count":28,"outputs":[{"output_type":"stream","text":["########################\n","logging outputs to  test\n","########################\n","Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n","Hopper-v2\n","Target value error 1.7256281503042193e-08 should be on the order of 1e-6 or lower\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NIk4S820DK_S"},"source":["For this section, we will also update our target network parameters as an exponential moving average of the critic parameters, instead of simply copying the current parameters periodically as in DQN. Generally, either method for target networks tends to work with appropriately chosen update rates.\n","\n","Fill out the update_target_parameter_ema method in <code>critics/bootstrapped_continuous_critic.py</code>."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rUtnNh2sDK_T","executionInfo":{"status":"ok","timestamp":1616697463305,"user_tz":-480,"elapsed":1055,"user":{"displayName":"Anthony Ng","photoUrl":"","userId":"16843483630260605487"}},"outputId":"6b7245cb-73ee-4fe7-b333-085c72dd9607"},"source":["# Test target network update\n","ac_args = dict(ac_base_args_dict)\n","\n","env_str = 'Hopper'\n","ac_args['env_name'] = '{}-v2'.format(env_str)\n","ac_args['entropy_weight'] = 0.1\n","actrainer = AC_Trainer(ac_args)\n","critic = actrainer.rl_trainer.agent.critic\n","\n","critic.target_update_rate = 0.5\n","\n","# at initialization, target and critic networks are the same\n","for p in critic.critic_network.parameters():\n","    p.data += 1.\n","    \n","critic.update_target_network_ema()\n","\n","for p, target_p in zip(critic.critic_network.parameters(), critic.target_network.parameters()):\n","    assert np.all(ptu.to_numpy((p-target_p)) == 0.5)"],"execution_count":52,"outputs":[{"output_type":"stream","text":["########################\n","logging outputs to  test\n","########################\n","Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n","Hopper-v2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1OFMy9XdDK_U"},"source":["Next, we will implement the actor update using the learned critic instead of Monte Carlo returns. \n","\n","To update our policy at a particular state $s$, our previous policy gradient (using the reward to go estimator) took a step on the objective (treating $Q^{\\pi}$ as a function that didn't depend on $\\pi$ and using the results of a single trajectory to estimate $Q^\\pi$)\n","$$\\mathbb E_{a \\sim \\pi_{\\theta}(s)}[Q^{\\pi_\\theta}(s, a)],$$\n","using the REINFORCE gradient estimator \n","$$\\mathbb E_{a \\sim \\pi_{\\theta}(s)}[\\nabla_{\\theta} \\log \\pi_{\\theta}(a\\vert s) Q^{\\pi}(s, a)].$$\n","This estimator only relied on the estimated value $Q^{\\pi_{\\theta}}(s,a)$, so was very general and could be applied with Monte Carlo estimates of $Q^{\\pi_\\theta}$.\n","\n","One way to estimate policy gradients with an actor critic algorithm would be to directly replace the Monte Carlo estimate of $Q$ with the learned critic $Q_{\\phi}$, and continue using the REINFORCE gradient estimator.\n","However, we note that we can explicitly compute derivatives of our learned critic $Q(s, a)$ with respect to the action $a$, which can enable potentially better gradient estimates. \n","\n","In order to take advantage of this, we would also need to differentiate sampled actions $a$ with respect to our policy parameters, which we can through a technique known as the _reparameterization trick_ or the _pathwise_ estimator. \n","The idea is that if our policy sampled actions according to $a \\sim \\mathcal N(\\mu_{\\theta}(s), \\sigma^2_{\\theta}(s))$, we can rewrite $a = f_{\\theta}(z)$, where $z \\sim \\mathcal N(0, 1)$, and $f(z) = \\mu_{\\theta}(s) + z \\cdot \\sigma_{\\theta}(s)$. Now all the randomness comes from sampling $z$, which doesn't depend on our policy, so we can now differentiate the sampled action $a$ with respect to our policy parameters $\\theta$ by simply differentiating through the function $f$ applied at the random noise $z$. \n","\n","Using the chain rule then allows to directly estimate gradients of \n","$$\\mathbb{E}_{a \\sim \\pi_{\\theta}(s)}[Q_{\\phi}(s,a)]$$\n","by drawing samples from $\\pi$ and differentiating $Q_{\\phi}(s,a)$ on the samples. \n","\n","Implement the actor update using this pathwise estimator in the update method of the MLPPolicyAC class in <code>policies/MLP_policy.py</code> (Hint: see the rsample function in for torch.distributions). Note that our implementation samples states uniformly from the entire replay buffer, not necessarily from the state distribution of the current policy. While this means we are no longer taking unbiased policy gradients (our estimates were already biased anyways due to using a learned critic), it works well in practice."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hVc8G-_VDK_V","executionInfo":{"status":"ok","timestamp":1616699962044,"user_tz":-480,"elapsed":1180,"user":{"displayName":"Anthony Ng","photoUrl":"","userId":"16843483630260605487"}},"outputId":"19a4ce39-815f-4d1b-e988-b3206a73fece"},"source":["# Compute actor update using the policy gradient. \n","# For this test to pass, make sure you only call sample once per actor update to not throw off \n","# the actor samples expected for the updates in the this test.\n","torch.manual_seed(0)\n","ac_dim = 2\n","ob_dim = 3\n","batch_size = 5\n","\n","np.random.seed(0)\n","obs = np.random.normal(size=(N, ob_dim))\n","\n","policy = MLPPolicyAC(\n","            ac_dim=ac_dim,\n","            ob_dim=ob_dim,\n","            n_layers=1,\n","            size=2,\n","            learning_rate=0.25,\n","            entropy_weight=0.)\n","\n","def dummy_critic(obs, acts):\n","    return torch.sum(acts + 1) + torch.sum(obs)\n","\n","initial_loss = policy.update(obs, dummy_critic)['Actor Training Loss']\n","expected_initial_loss = -17.083496\n","\n","print(\"Initial loss error\", rel_error(expected_initial_loss, initial_loss), \"should be on the order of 1e-6 or less.\")\n","for i in range(5):\n","    loss = policy.update(obs, dummy_critic)['Actor Training Loss']\n","    print(loss)\n","\n","expected_final_loss = -30.103575\n","\n","print(\"Final loss error\", rel_error(expected_final_loss, loss), \"should be on the order of 1e-6 or less.\")\n","    \n"],"execution_count":60,"outputs":[{"output_type":"stream","text":["Initial loss error 2.7438762975115168e-09 should be on the order of 1e-6 or less.\n","-17.314678\n","-22.621984\n","-25.828485\n","-29.670477\n","-30.103575\n","Final loss error 4.105698129439902e-09 should be on the order of 1e-6 or less.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"enJGdK3fDK_W"},"source":["Now we'll train our actor critic agent on the HalfCheetah task. You should see your policies generally get over 600 returns. \n","\n","We note that these actor critic algorithms, since they make use of off-policy updates, can be much more sample efficient than the basic policy gradient algorith we saw earlier. In our actor critic algorithms here, we only take 1000 new samples from the environment per iteration, while the policy gradient algorithms often needed many more samples per iteration to estimate the Monte Carlo returns (for example, we used 10000 in the Hopper experiments with policy gradient).\n","\n"]},{"cell_type":"code","metadata":{"id":"PwN7AVl4DK_W"},"source":["ac_args = dict(ac_base_args_dict)\n","\n","env_str = 'HalfCheetah'\n","ac_args['env_name'] = '{}-v2'.format(env_str)\n","ac_args['n_iter'] = 50\n","\n","# Delete all previous logs\n","remove_folder('logs/actor_critic/{}/no_entropy_reg'.format(env_str))\n","\n","for seed in range(3):\n","    print(\"Running actor critic experiment with seed\", seed)\n","    ac_args['seed'] = seed\n","    ac_args['logdir'] = 'logs/actor_critic/{}/seed{}'.format(env_str, seed)\n","    actrainer = AC_Trainer(ac_args)\n","    actrainer.run_training_loop()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z_4hRqI7DK_X"},"source":["### Visualize Actor Critic results on Halfheetah\n","%load_ext tensorboard\n","%tensorboard --logdir logs/actor_critic/HalfCheetah"],"execution_count":null,"outputs":[]}]}