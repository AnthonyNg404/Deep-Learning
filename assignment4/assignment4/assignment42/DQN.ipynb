{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"DQN.ipynb","provenance":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"dsy2sudNDKL9"},"source":["# Deep Q Networks\n","Previously, we trained policies using policy gradient algorithms, which directly estimated the gradient of the returns for the policy and performed stochastic gradient ascent. In this section, we will now implement deep Q-learning, which does not explicitly optimize a policy, but simply infers the policy from a learned Q function that is trained via dynamic programming.\n","\n","We will assume discrete action spaces for this notebook as to enable us to easily select actions that maximize the Q-function at given states."],"id":"dsy2sudNDKL9"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"617if6M1FSZM","executionInfo":{"status":"ok","timestamp":1616670743113,"user_tz":-480,"elapsed":17645,"user":{"displayName":"Anthony Ng","photoUrl":"","userId":"16843483630260605487"}},"outputId":"3b523888-dbc7-46f2-9f68-2969422be9b8"},"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","DRIVE_PATH = '/content/drive/My\\ Drive/282'"],"id":"617if6M1FSZM","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sOd26ZqiDKMF","executionInfo":{"status":"ok","timestamp":1616670887744,"user_tz":-480,"elapsed":10158,"user":{"displayName":"Anthony Ng","photoUrl":"","userId":"16843483630260605487"}}},"source":["# As usual, a bit of setup\n","import os\n","import shutil\n","import time\n","import torch\n","import numpy as np\n","\n","import deeprl.infrastructure.pytorch_util as ptu\n","\n","from deeprl.infrastructure.rl_trainer import RL_Trainer\n","from deeprl.infrastructure.trainers import PG_Trainer\n","from deeprl.infrastructure.trainers import DQN_Trainer\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","def rel_error(x, y):\n","    \"\"\" returns relative error \"\"\"\n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n","\n","def remove_folder(path):\n","    # check if folder exists\n","    if os.path.exists(path): \n","        print(\"Clearing old results at {}\".format(path))\n","        # remove if exists\n","        shutil.rmtree(path)\n","    else:\n","        print(\"Folder {} does not exist yet. No old results to delete\".format(path))"],"id":"sOd26ZqiDKMF","execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"0K7dUX8tDKMG","executionInfo":{"status":"ok","timestamp":1616670888036,"user_tz":-480,"elapsed":10444,"user":{"displayName":"Anthony Ng","photoUrl":"","userId":"16843483630260605487"}}},"source":["dqn_base_args_dict = dict(\n","    env_name = 'LunarLander-v3', #@param \n","    exp_name = 'test_dqn', #@param\n","    save_params = False, #@param {type: \"boolean\"}\n","    \n","    ## PDF will tell you how to set ep_len\n","    ## and discount for each environment\n","    ep_len = 200, #@param {type: \"integer\"}\n","    # discount = 0.95, #@param {type: \"number\"}\n","\n","    # Training\n","    num_agent_train_steps_per_iter = 1, #@param {type: \"integer\"})\n","    num_critic_updates_per_agent_update = 1, #@param {type: \"integer\"}\n","  \n","    #@markdown Q-learning parameters\n","    double_q = False, #@param {type: \"boolean\"}\n","\n","    # batches & buffers\n","    batch_size = 32, #@param {type: \"integer\"})\n","    batch_size_initial=1000,\n","\n","    #@markdown logging\n","    video_log_freq = -1, #@param {type: \"integer\"}\n","    scalar_log_freq = 1000, #@param {type: \"integer\"}\n","\n","    #@markdown gpu & run-time settings\n","    no_gpu = False, #@param {type: \"boolean\"}\n","    which_gpu = 0, #@param {type: \"integer\"}\n","    seed = 2, #@param {type: \"integer\"}\n","    logdir = 'test',\n",")"],"id":"0K7dUX8tDKMG","execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w_DSI3qFDKMH"},"source":["## DQN updates\n","Recall in Q-learning, we attempt to solve the optimal state-action values (which we refer to as Q-values $Q(s,a)$), by finding solutions to the Bellman equation given by\n","$$Q(s,a) = r(s,a) + \\gamma \\mathbb{E}_{s' \\sim p(s'\\vert s,a)}[\\max_{a'}Q(s', a')].$$\n","\n","Regular tabular Q-learning would take sample transitions $(s, a, r, s')$ and perform updates according to\n","$$Q(s,a) \\leftarrow Q(s,a) + \\alpha (r(s,a) + \\gamma \\max_{a'} Q(s', a') - Q(s,a)),$$\n","where $\\alpha$ is a stepsize parameter.\n","\n","This can be interpreted as updating $Q(s,a)$ by taking one gradient step on a squared Bellman error objective\n","$$(r(s, a) +\\gamma \\max_{a'} \\tilde Q(s', a') - Q(s,a))^2,$$\n","where $\\tilde Q$ is a copy of $Q$, but is not differentiated when taking the gradient step.\n","\n","Adapting this update to the setting where we use a neural network with parameters $\\theta$ to approximate $Q(s,a)$, we then train $\\theta$ with the loss function \n","$$\\min_{\\theta} \\mathbb{E}_{s, a, s' \\sim D} [L(Q_{\\theta}(s,a), r(s,a) + \\gamma \\max_{a'} Q_{\\tilde \\theta}(s', a'))]$$\n","where $D$ is our replay buffer containing past transitions we've experienced, $L$ is some loss function capturing how far the predicted Q-values are from the target values, and $\\tilde \\theta$ are the target Q function parameters, which are usually a delayed copy of $\\theta$ for stability reasons.\n","\n","We note our previous policy gradient algorithms were _on-policy_ algoritms, which meant they updated the policy using only the data collected from the most recent policy, and discard all the data after using it just once. In contrast, DQN uses _off-policy_ updates by sampling data from all past interactions, allowing for data reuse over time."],"id":"w_DSI3qFDKMH"},{"cell_type":"markdown","metadata":{"id":"Rbn2u30sDKMI"},"source":["Fill out the missing components for the basic Q-learning update in <code>critics/dqn_critic.py</code> (not including the double_q section)."],"id":"Rbn2u30sDKMI"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2LuaUveIDKMI","executionInfo":{"status":"ok","timestamp":1616682772055,"user_tz":-480,"elapsed":760,"user":{"displayName":"Anthony Ng","photoUrl":"","userId":"16843483630260605487"}},"outputId":"fcb227e4-7b43-4d4e-8eed-aac6f6e40869"},"source":["#### Test DQN updates\n","dqn_args = dict(dqn_base_args_dict)\n","\n","env_str = 'LunarLander'\n","dqn_args['env_name'] = '{}-v3'.format(env_str)\n","dqn_args['double_q'] = False\n","dqntrainer = DQN_Trainer(dqn_args)\n","dqnagent = dqntrainer.rl_trainer.agent\n","critic = dqnagent.critic\n","\n","ob_dim = critic.ob_dim\n","ac_dim = 6\n","N = 5\n","\n","np.random.seed(0)\n","obs = np.random.normal(size=(N, ob_dim))\n","acts = np.random.choice(ac_dim, size=(N,))\n","next_obs = np.random.normal(size=(N, ob_dim))\n","rewards = np.random.normal(size=N)\n","terminals = np.zeros(N)\n","terminals[0] = 1\n","\n","first_weight_before = np.array(ptu.to_numpy(next(critic.q_net.parameters())))\n","print(\"Weight before update (first row)\", first_weight_before[0])\n","\n","\n","loss = critic.update(obs, acts, next_obs, rewards, terminals)['Training Loss']\n","expected_loss = 0.9408444\n","loss_error = rel_error(loss, expected_loss)\n","print(\"Initial loss\", loss)\n","print(\"Initial Loss Error\", loss_error, \"should be on the order of 1e-6 or lower\")\n","\n","for i in range(4):\n","    loss = critic.update(obs, acts, next_obs, rewards, terminals)['Training Loss']\n","    print(loss)\n","\n","expected_loss = 0.7889254\n","loss_error = rel_error(loss, expected_loss)\n","print(\"Loss Error\", loss_error, \"should be on the order of 1e-6 or lower\")\n","\n","\n","first_weight_after = np.array(ptu.to_numpy(next(critic.q_net.parameters())))\n","print(\"Weight after update (first row)\", first_weight_after.shape)\n","# Test DQN gradient\n","print(first_weight_after[0])\n","weight_change_partial = first_weight_after[0] - first_weight_before[0]\n","expected_weight_change = np.array([-0.00491365, -0.00500049, -0.00499149, -0.00491229, -0.00490125,  0.00489534,\n"," -0.00282785, -0.00171614,  0.00485604])\n","\n","\n","updated_weight_error = rel_error(weight_change_partial, expected_weight_change)\n","print(\"Weight Update Error\", updated_weight_error, \"should be on the order of 1e-6 or lower\")"],"id":"2LuaUveIDKMI","execution_count":43,"outputs":[{"output_type":"stream","text":["########################\n","logging outputs to  test\n","########################\n","Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n","LunarLander-v3\n","Weight before update (first row) [ 0.07646339 -0.07932477  0.09140956 -0.01702595  0.1423959   0.07935759\n"," -0.03831156 -0.2694876   0.0761048 ]\n","Initial loss 0.9408444\n","Initial Loss Error 8.831612855468697e-09 should be on the order of 1e-6 or lower\n","0.9007309\n","0.8621321\n","0.82467556\n","0.7889254\n","Loss Error 5.904878045285727e-09 should be on the order of 1e-6 or lower\n","Weight after update (first row) (64, 9)\n","[ 0.07154974 -0.08432526  0.08641808 -0.02193824  0.13749465  0.08425292\n"," -0.04113941 -0.27120373  0.08096085]\n","Weight Update Error 8.937585787696078e-07 should be on the order of 1e-6 or lower\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"MiJZFZETDKMJ"},"source":["Implement the missing components in the get_action method of <code>policies/argmax_policy.py</code> and the step_env method in <code>agents/dqn_agent.py</code> to allow our agent to interact with the environment."],"id":"MiJZFZETDKMJ"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GNEvNyoaDKMK","executionInfo":{"status":"ok","timestamp":1616688100481,"user_tz":-480,"elapsed":1367,"user":{"displayName":"Anthony Ng","photoUrl":"","userId":"16843483630260605487"}},"outputId":"aba370a7-eba1-4baa-84d3-74d0867d3647"},"source":["### Test argmax policy\n","dqn_args = dict(dqn_base_args_dict)\n","\n","env_str = 'LunarLander'\n","dqn_args['env_name'] = '{}-v3'.format(env_str)\n","dqn_args['double_q'] = False\n","dqntrainer = DQN_Trainer(dqn_args)\n","dqnagent = dqntrainer.rl_trainer.agent\n","actor = dqnagent.actor\n","\n","ob_dim = critic.ob_dim\n","ac_dim = 6\n","N = 5\n","\n","np.random.seed(0)\n","obs = np.random.normal(size=(N, ob_dim))\n","\n","actions = actor.get_action(obs)\n","correct_actions = np.array([1, 0, 1, 0, 1])\n","\n","assert np.all(correct_actions == actions)"],"id":"GNEvNyoaDKMK","execution_count":61,"outputs":[{"output_type":"stream","text":["########################\n","logging outputs to  test\n","########################\n","Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n","LunarLander-v3\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"e4F-SmRUDKML"},"source":["We can now test our DQN implementation on the LunarLander environment. These experiments can take a while to run (over 10 minutes per seed) on CPU, so start early."],"id":"e4F-SmRUDKML"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GxkTN0PRDKML","outputId":"a15c8c51-93cb-4ab1-fad1-a4b059e78126"},"source":["dqn_args = dict(dqn_base_args_dict)\n","\n","env_str = 'LunarLander'\n","dqn_args['env_name'] = '{}-v3'.format(env_str)\n","dqn_args['double_q'] = False\n","\n","# Delete all previous logs\n","remove_folder('logs/dqn/{}/vanilla_dqn'.format(env_str))\n","\n","for seed in range(3):\n","    print(\"Running DQN experiment with seed\", seed)\n","    dqn_args['seed'] = seed\n","    dqn_args['logdir'] = 'logs/dqn/{}/vanilla_dqn/seed{}'.format(env_str, seed)\n","    dqntrainer = DQN_Trainer(dqn_args)\n","    dqntrainer.run_training_loop()"],"id":"GxkTN0PRDKML","execution_count":null,"outputs":[{"output_type":"stream","text":["Clearing old results at logs/dqn/LunarLander/vanilla_dqn\n","Running DQN experiment with seed 0\n","########################\n","logging outputs to  logs/dqn/LunarLander/vanilla_dqn/seed0\n","########################\n","Using CPU for this assignment. There may be some bugs with using GPU that cause test cases to not match. You can uncomment the code below if you want to try using it.\n","LunarLander-v3\n","\n","\n","********** Iteration 0 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1\n","mean reward (100 episodes) nan\n","best mean reward -inf\n","running time 0.002473\n","Train_EnvstepsSoFar : 1\n","TimeSinceStart : 0.0024726390838623047\n","Done logging...\n","\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"],"name":"stderr"},{"output_type":"stream","text":["\n","\n","********** Iteration 1000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 1001\n","mean reward (100 episodes) -361.464801\n","best mean reward -inf\n","running time 0.692687\n","Train_EnvstepsSoFar : 1001\n","Train_AverageReturn : -361.46480063692485\n","TimeSinceStart : 0.6926872730255127\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 2000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 2001\n","mean reward (100 episodes) -356.064325\n","best mean reward -inf\n","running time 3.577993\n","Train_EnvstepsSoFar : 2001\n","Train_AverageReturn : -356.06432532472695\n","TimeSinceStart : 3.577993154525757\n","Training Loss : 0.23083141446113586\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 3000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 3001\n","mean reward (100 episodes) -311.599968\n","best mean reward -inf\n","running time 6.179479\n","Train_EnvstepsSoFar : 3001\n","Train_AverageReturn : -311.5999676699081\n","TimeSinceStart : 6.179478645324707\n","Training Loss : 0.25829875469207764\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 4000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 4001\n","mean reward (100 episodes) -290.405298\n","best mean reward -inf\n","running time 8.835816\n","Train_EnvstepsSoFar : 4001\n","Train_AverageReturn : -290.40529775974494\n","TimeSinceStart : 8.835815906524658\n","Training Loss : 3.2054367065429688\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 5000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 5001\n","mean reward (100 episodes) -289.885879\n","best mean reward -inf\n","running time 11.620258\n","Train_EnvstepsSoFar : 5001\n","Train_AverageReturn : -289.8858791275752\n","TimeSinceStart : 11.62025761604309\n","Training Loss : 0.3398244380950928\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 6000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 6001\n","mean reward (100 episodes) -287.721903\n","best mean reward -inf\n","running time 14.509539\n","Train_EnvstepsSoFar : 6001\n","Train_AverageReturn : -287.72190346767144\n","TimeSinceStart : 14.509539127349854\n","Training Loss : 0.35096678137779236\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 7000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 7001\n","mean reward (100 episodes) -280.788172\n","best mean reward -inf\n","running time 17.634439\n","Train_EnvstepsSoFar : 7001\n","Train_AverageReturn : -280.78817237042267\n","TimeSinceStart : 17.634438514709473\n","Training Loss : 0.45847299695014954\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 8000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 8001\n","mean reward (100 episodes) -266.189680\n","best mean reward -inf\n","running time 20.518912\n","Train_EnvstepsSoFar : 8001\n","Train_AverageReturn : -266.18967986642065\n","TimeSinceStart : 20.518911838531494\n","Training Loss : 0.26132574677467346\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 9000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 9001\n","mean reward (100 episodes) -263.294688\n","best mean reward -inf\n","running time 23.352851\n","Train_EnvstepsSoFar : 9001\n","Train_AverageReturn : -263.29468770032497\n","TimeSinceStart : 23.352851152420044\n","Training Loss : 0.8923195600509644\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 10000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 10001\n","mean reward (100 episodes) -257.868982\n","best mean reward -inf\n","running time 26.366602\n","Train_EnvstepsSoFar : 10001\n","Train_AverageReturn : -257.868982011018\n","TimeSinceStart : 26.366602420806885\n","Training Loss : 0.7248114347457886\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 11000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 11001\n","mean reward (100 episodes) -252.356306\n","best mean reward -inf\n","running time 29.433851\n","Train_EnvstepsSoFar : 11001\n","Train_AverageReturn : -252.3563056264166\n","TimeSinceStart : 29.43385100364685\n","Training Loss : 0.45861703157424927\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 12000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 12001\n","mean reward (100 episodes) -245.685083\n","best mean reward -inf\n","running time 32.775753\n","Train_EnvstepsSoFar : 12001\n","Train_AverageReturn : -245.68508346678362\n","TimeSinceStart : 32.775752782821655\n","Training Loss : 0.35099855065345764\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 13000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 13001\n","mean reward (100 episodes) -242.268223\n","best mean reward -inf\n","running time 36.154239\n","Train_EnvstepsSoFar : 13001\n","Train_AverageReturn : -242.26822272366687\n","TimeSinceStart : 36.15423917770386\n","Training Loss : 0.1893889605998993\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 14000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 14001\n","mean reward (100 episodes) -230.331176\n","best mean reward -inf\n","running time 39.263827\n","Train_EnvstepsSoFar : 14001\n","Train_AverageReturn : -230.33117648472552\n","TimeSinceStart : 39.263827085494995\n","Training Loss : 1.01033616065979\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 15000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 15001\n","mean reward (100 episodes) -226.904451\n","best mean reward -inf\n","running time 43.896132\n","Train_EnvstepsSoFar : 15001\n","Train_AverageReturn : -226.90445054063989\n","TimeSinceStart : 43.896132469177246\n","Training Loss : 0.3917540907859802\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 16000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 16001\n","mean reward (100 episodes) -225.089447\n","best mean reward -inf\n","running time 47.368884\n","Train_EnvstepsSoFar : 16001\n","Train_AverageReturn : -225.0894469065005\n","TimeSinceStart : 47.36888384819031\n","Training Loss : 0.5151476860046387\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 17000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 17001\n","mean reward (100 episodes) -218.615004\n","best mean reward -218.615004\n","running time 52.181582\n","Train_EnvstepsSoFar : 17001\n","Train_AverageReturn : -218.61500391430255\n","Train_BestReturn : -218.61500391430255\n","TimeSinceStart : 52.18158221244812\n","Training Loss : 0.5224773287773132\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 18000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 18001\n","mean reward (100 episodes) -217.211331\n","best mean reward -217.211331\n","running time 57.180145\n","Train_EnvstepsSoFar : 18001\n","Train_AverageReturn : -217.21133087800078\n","Train_BestReturn : -217.21133087800078\n","TimeSinceStart : 57.18014454841614\n","Training Loss : 0.8599382638931274\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 19000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 19001\n","mean reward (100 episodes) -215.603329\n","best mean reward -215.603329\n","running time 63.002998\n","Train_EnvstepsSoFar : 19001\n","Train_AverageReturn : -215.60332898643642\n","Train_BestReturn : -215.60332898643642\n","TimeSinceStart : 63.002997636795044\n","Training Loss : 0.38553136587142944\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 20000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 20001\n","mean reward (100 episodes) -210.277586\n","best mean reward -210.277586\n","running time 66.652647\n","Train_EnvstepsSoFar : 20001\n","Train_AverageReturn : -210.2775856943242\n","Train_BestReturn : -210.2775856943242\n","TimeSinceStart : 66.6526472568512\n","Training Loss : 0.49898266792297363\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 21000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 21001\n","mean reward (100 episodes) -208.563069\n","best mean reward -208.563069\n","running time 71.070387\n","Train_EnvstepsSoFar : 21001\n","Train_AverageReturn : -208.56306936288115\n","Train_BestReturn : -208.56306936288115\n","TimeSinceStart : 71.0703866481781\n","Training Loss : 1.7444257736206055\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 22000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 22001\n","mean reward (100 episodes) -201.338875\n","best mean reward -201.338875\n","running time 75.566042\n","Train_EnvstepsSoFar : 22001\n","Train_AverageReturn : -201.33887459601777\n","Train_BestReturn : -201.33887459601777\n","TimeSinceStart : 75.56604194641113\n","Training Loss : 1.6822632551193237\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 23000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 23001\n","mean reward (100 episodes) -179.670851\n","best mean reward -179.670851\n","running time 78.560500\n","Train_EnvstepsSoFar : 23001\n","Train_AverageReturn : -179.6708513373398\n","Train_BestReturn : -179.6708513373398\n","TimeSinceStart : 78.56049990653992\n","Training Loss : 0.7091899514198303\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 24000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 24001\n","mean reward (100 episodes) -179.700269\n","best mean reward -179.670851\n","running time 83.203011\n","Train_EnvstepsSoFar : 24001\n","Train_AverageReturn : -179.70026865900877\n","Train_BestReturn : -179.6708513373398\n","TimeSinceStart : 83.20301103591919\n","Training Loss : 0.9658647179603577\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 25000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 25001\n","mean reward (100 episodes) -177.011954\n","best mean reward -177.011954\n","running time 89.788725\n","Train_EnvstepsSoFar : 25001\n","Train_AverageReturn : -177.01195442724057\n","Train_BestReturn : -177.01195442724057\n","TimeSinceStart : 89.78872466087341\n","Training Loss : 0.8277741074562073\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 26000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 26001\n","mean reward (100 episodes) -176.097591\n","best mean reward -176.097591\n","running time 94.523554\n","Train_EnvstepsSoFar : 26001\n","Train_AverageReturn : -176.09759055349934\n","Train_BestReturn : -176.09759055349934\n","TimeSinceStart : 94.5235538482666\n","Training Loss : 0.9758857488632202\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 27000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 27001\n","mean reward (100 episodes) -177.151547\n","best mean reward -176.097591\n","running time 99.804746\n","Train_EnvstepsSoFar : 27001\n","Train_AverageReturn : -177.1515473083504\n","Train_BestReturn : -176.09759055349934\n","TimeSinceStart : 99.80474638938904\n","Training Loss : 1.0038903951644897\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 28000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 28001\n","mean reward (100 episodes) -176.627963\n","best mean reward -176.097591\n","running time 105.130658\n","Train_EnvstepsSoFar : 28001\n","Train_AverageReturn : -176.6279632331235\n","Train_BestReturn : -176.09759055349934\n","TimeSinceStart : 105.13065767288208\n","Training Loss : 1.459631323814392\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 29000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 29001\n","mean reward (100 episodes) -175.374228\n","best mean reward -175.374228\n","running time 109.979908\n","Train_EnvstepsSoFar : 29001\n","Train_AverageReturn : -175.374228394366\n","Train_BestReturn : -175.374228394366\n","TimeSinceStart : 109.97990798950195\n","Training Loss : 2.010913133621216\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 30000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 30001\n","mean reward (100 episodes) -175.635987\n","best mean reward -175.374228\n","running time 114.427825\n","Train_EnvstepsSoFar : 30001\n","Train_AverageReturn : -175.63598735334472\n","Train_BestReturn : -175.374228394366\n","TimeSinceStart : 114.42782521247864\n","Training Loss : 1.9610780477523804\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 31000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 31001\n","mean reward (100 episodes) -174.623931\n","best mean reward -174.623931\n","running time 119.306499\n","Train_EnvstepsSoFar : 31001\n","Train_AverageReturn : -174.62393142220742\n","Train_BestReturn : -174.62393142220742\n","TimeSinceStart : 119.30649900436401\n","Training Loss : 0.43892040848731995\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 32000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 32001\n","mean reward (100 episodes) -171.766207\n","best mean reward -171.766207\n","running time 123.829611\n","Train_EnvstepsSoFar : 32001\n","Train_AverageReturn : -171.7662069641761\n","Train_BestReturn : -171.7662069641761\n","TimeSinceStart : 123.82961082458496\n","Training Loss : 3.4196252822875977\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 33000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 33001\n","mean reward (100 episodes) -171.168302\n","best mean reward -171.168302\n","running time 129.094513\n","Train_EnvstepsSoFar : 33001\n","Train_AverageReturn : -171.1683019082129\n","Train_BestReturn : -171.1683019082129\n","TimeSinceStart : 129.09451293945312\n","Training Loss : 0.6502508521080017\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 34000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 34001\n","mean reward (100 episodes) -169.910447\n","best mean reward -169.910447\n","running time 134.499302\n","Train_EnvstepsSoFar : 34001\n","Train_AverageReturn : -169.91044703241968\n","Train_BestReturn : -169.91044703241968\n","TimeSinceStart : 134.4993016719818\n","Training Loss : 0.337772935628891\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 35000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 35001\n","mean reward (100 episodes) -169.049547\n","best mean reward -169.049547\n","running time 138.729736\n","Train_EnvstepsSoFar : 35001\n","Train_AverageReturn : -169.04954692079332\n","Train_BestReturn : -169.04954692079332\n","TimeSinceStart : 138.72973561286926\n","Training Loss : 0.31150904297828674\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 36000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 36001\n","mean reward (100 episodes) -168.904046\n","best mean reward -168.904046\n","running time 142.919295\n","Train_EnvstepsSoFar : 36001\n","Train_AverageReturn : -168.90404574136898\n","Train_BestReturn : -168.90404574136898\n","TimeSinceStart : 142.91929483413696\n","Training Loss : 0.3851931393146515\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 37000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 37001\n","mean reward (100 episodes) -168.261356\n","best mean reward -168.261356\n","running time 146.895838\n","Train_EnvstepsSoFar : 37001\n","Train_AverageReturn : -168.26135576189282\n","Train_BestReturn : -168.26135576189282\n","TimeSinceStart : 146.89583802223206\n","Training Loss : 0.749341607093811\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 38000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 38001\n","mean reward (100 episodes) -166.351632\n","best mean reward -166.351632\n","running time 150.836141\n","Train_EnvstepsSoFar : 38001\n","Train_AverageReturn : -166.35163229795097\n","Train_BestReturn : -166.35163229795097\n","TimeSinceStart : 150.83614087104797\n","Training Loss : 0.2807789146900177\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 39000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 39001\n","mean reward (100 episodes) -164.595396\n","best mean reward -164.595396\n","running time 155.174775\n","Train_EnvstepsSoFar : 39001\n","Train_AverageReturn : -164.59539568892134\n","Train_BestReturn : -164.59539568892134\n","TimeSinceStart : 155.1747751235962\n","Training Loss : 0.2245982438325882\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 40000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 40001\n","mean reward (100 episodes) -160.743555\n","best mean reward -160.743555\n","running time 160.075913\n","Train_EnvstepsSoFar : 40001\n","Train_AverageReturn : -160.7435545049633\n","Train_BestReturn : -160.7435545049633\n","TimeSinceStart : 160.07591319084167\n","Training Loss : 0.4771993160247803\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 41000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 41001\n","mean reward (100 episodes) -160.229867\n","best mean reward -160.229867\n","running time 165.201851\n","Train_EnvstepsSoFar : 41001\n","Train_AverageReturn : -160.22986730362192\n","Train_BestReturn : -160.22986730362192\n","TimeSinceStart : 165.20185136795044\n","Training Loss : 0.5120646953582764\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 42000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 42001\n","mean reward (100 episodes) -158.688436\n","best mean reward -158.688436\n","running time 169.305866\n","Train_EnvstepsSoFar : 42001\n","Train_AverageReturn : -158.6884355913583\n","Train_BestReturn : -158.6884355913583\n","TimeSinceStart : 169.30586624145508\n","Training Loss : 0.37031763792037964\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 43000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 43001\n","mean reward (100 episodes) -155.444615\n","best mean reward -155.444615\n","running time 174.015866\n","Train_EnvstepsSoFar : 43001\n","Train_AverageReturn : -155.44461532738381\n","Train_BestReturn : -155.44461532738381\n","TimeSinceStart : 174.0158658027649\n","Training Loss : 1.7120013236999512\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 44000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 44001\n","mean reward (100 episodes) -152.578556\n","best mean reward -152.578556\n","running time 179.535726\n","Train_EnvstepsSoFar : 44001\n","Train_AverageReturn : -152.57855639455383\n","Train_BestReturn : -152.57855639455383\n","TimeSinceStart : 179.53572583198547\n","Training Loss : 1.3516004085540771\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 45000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 45001\n","mean reward (100 episodes) -150.332346\n","best mean reward -150.332346\n","running time 184.075958\n","Train_EnvstepsSoFar : 45001\n","Train_AverageReturn : -150.33234554702315\n","Train_BestReturn : -150.33234554702315\n","TimeSinceStart : 184.0759575366974\n","Training Loss : 2.7066667079925537\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 46000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 46001\n","mean reward (100 episodes) -147.673914\n","best mean reward -147.673914\n","running time 188.618255\n","Train_EnvstepsSoFar : 46001\n","Train_AverageReturn : -147.67391356601615\n","Train_BestReturn : -147.67391356601615\n","TimeSinceStart : 188.6182553768158\n","Training Loss : 0.36585789918899536\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 47000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 47001\n","mean reward (100 episodes) -148.467907\n","best mean reward -147.673914\n","running time 192.718075\n","Train_EnvstepsSoFar : 47001\n","Train_AverageReturn : -148.4679068718706\n","Train_BestReturn : -147.67391356601615\n","TimeSinceStart : 192.71807527542114\n","Training Loss : 1.6989948749542236\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 48000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 48001\n","mean reward (100 episodes) -148.278213\n","best mean reward -147.673914\n","running time 197.097143\n","Train_EnvstepsSoFar : 48001\n","Train_AverageReturn : -148.27821333054823\n","Train_BestReturn : -147.67391356601615\n","TimeSinceStart : 197.09714341163635\n","Training Loss : 0.26231837272644043\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 49000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 49001\n","mean reward (100 episodes) -143.491489\n","best mean reward -143.491489\n","running time 201.258134\n","Train_EnvstepsSoFar : 49001\n","Train_AverageReturn : -143.4914888717588\n","Train_BestReturn : -143.4914888717588\n","TimeSinceStart : 201.2581343650818\n","Training Loss : 0.2758738696575165\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 50000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 50001\n","mean reward (100 episodes) -140.219066\n","best mean reward -140.219066\n","running time 205.131971\n","Train_EnvstepsSoFar : 50001\n","Train_AverageReturn : -140.21906636743844\n","Train_BestReturn : -140.21906636743844\n","TimeSinceStart : 205.13197088241577\n","Training Loss : 0.5697697401046753\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 51000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 51001\n","mean reward (100 episodes) -137.723303\n","best mean reward -137.723303\n","running time 209.234681\n","Train_EnvstepsSoFar : 51001\n","Train_AverageReturn : -137.72330320922052\n","Train_BestReturn : -137.72330320922052\n","TimeSinceStart : 209.23468136787415\n","Training Loss : 0.3345071077346802\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 52000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 52001\n","mean reward (100 episodes) -134.568538\n","best mean reward -134.568538\n","running time 215.091100\n","Train_EnvstepsSoFar : 52001\n","Train_AverageReturn : -134.5685376653322\n","Train_BestReturn : -134.5685376653322\n","TimeSinceStart : 215.0910997390747\n","Training Loss : 1.2035490274429321\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 53000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 53001\n","mean reward (100 episodes) -127.522489\n","best mean reward -127.522489\n","running time 218.888463\n","Train_EnvstepsSoFar : 53001\n","Train_AverageReturn : -127.52248865914564\n","Train_BestReturn : -127.52248865914564\n","TimeSinceStart : 218.88846349716187\n","Training Loss : 0.9531756639480591\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 54000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 54001\n","mean reward (100 episodes) -127.249958\n","best mean reward -127.249958\n","running time 223.082857\n","Train_EnvstepsSoFar : 54001\n","Train_AverageReturn : -127.24995836110037\n","Train_BestReturn : -127.24995836110037\n","TimeSinceStart : 223.082857131958\n","Training Loss : 0.25875377655029297\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 55000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 55001\n","mean reward (100 episodes) -127.638820\n","best mean reward -127.249958\n","running time 227.165277\n","Train_EnvstepsSoFar : 55001\n","Train_AverageReturn : -127.63882048483089\n","Train_BestReturn : -127.24995836110037\n","TimeSinceStart : 227.16527652740479\n","Training Loss : 2.3031418323516846\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 56000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 56001\n","mean reward (100 episodes) -126.174664\n","best mean reward -126.174664\n","running time 230.778454\n","Train_EnvstepsSoFar : 56001\n","Train_AverageReturn : -126.17466403541917\n","Train_BestReturn : -126.17466403541917\n","TimeSinceStart : 230.7784538269043\n","Training Loss : 0.2312711477279663\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 57000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 57001\n","mean reward (100 episodes) -125.524247\n","best mean reward -125.524247\n","running time 236.127934\n","Train_EnvstepsSoFar : 57001\n","Train_AverageReturn : -125.52424710007541\n","Train_BestReturn : -125.52424710007541\n","TimeSinceStart : 236.12793374061584\n","Training Loss : 0.15868107974529266\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 58000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 58001\n","mean reward (100 episodes) -125.557510\n","best mean reward -125.524247\n","running time 242.080689\n","Train_EnvstepsSoFar : 58001\n","Train_AverageReturn : -125.55751025989552\n","Train_BestReturn : -125.52424710007541\n","TimeSinceStart : 242.08068871498108\n","Training Loss : 0.19232670962810516\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 59000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 59001\n","mean reward (100 episodes) -125.465500\n","best mean reward -125.465500\n","running time 245.595071\n","Train_EnvstepsSoFar : 59001\n","Train_AverageReturn : -125.46550022336027\n","Train_BestReturn : -125.46550022336027\n","TimeSinceStart : 245.59507060050964\n","Training Loss : 0.2669869661331177\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 60000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 60001\n","mean reward (100 episodes) -123.748078\n","best mean reward -123.748078\n","running time 249.490965\n","Train_EnvstepsSoFar : 60001\n","Train_AverageReturn : -123.74807751687553\n","Train_BestReturn : -123.74807751687553\n","TimeSinceStart : 249.49096536636353\n","Training Loss : 0.2847729027271271\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 61000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 61001\n","mean reward (100 episodes) -115.586753\n","best mean reward -115.586753\n","running time 253.428863\n","Train_EnvstepsSoFar : 61001\n","Train_AverageReturn : -115.58675268972591\n","Train_BestReturn : -115.58675268972591\n","TimeSinceStart : 253.42886304855347\n","Training Loss : 0.6241478323936462\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 62000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 62001\n","mean reward (100 episodes) -113.109926\n","best mean reward -113.109926\n","running time 257.438775\n","Train_EnvstepsSoFar : 62001\n","Train_AverageReturn : -113.1099255587977\n","Train_BestReturn : -113.1099255587977\n","TimeSinceStart : 257.4387753009796\n","Training Loss : 0.34487131237983704\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 63000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 63001\n","mean reward (100 episodes) -112.221707\n","best mean reward -112.221707\n","running time 260.882291\n","Train_EnvstepsSoFar : 63001\n","Train_AverageReturn : -112.221707421243\n","Train_BestReturn : -112.221707421243\n","TimeSinceStart : 260.88229060173035\n","Training Loss : 1.3532979488372803\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 64000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 64001\n","mean reward (100 episodes) -111.305627\n","best mean reward -111.305627\n","running time 265.538828\n","Train_EnvstepsSoFar : 64001\n","Train_AverageReturn : -111.30562684118655\n","Train_BestReturn : -111.30562684118655\n","TimeSinceStart : 265.5388283729553\n","Training Loss : 0.4334552586078644\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 65000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 65001\n","mean reward (100 episodes) -110.576037\n","best mean reward -110.576037\n","running time 270.821846\n","Train_EnvstepsSoFar : 65001\n","Train_AverageReturn : -110.57603717513149\n","Train_BestReturn : -110.57603717513149\n","TimeSinceStart : 270.82184648513794\n","Training Loss : 0.45673540234565735\n","Done logging...\n","\n","\n","\n","\n","********** Iteration 66000 ************\n","\n","Training agent...\n","\n","Beginning logging procedure...\n","Timestep 66001\n","mean reward (100 episodes) -109.576673\n","best mean reward -109.576673\n","running time 276.129093\n","Train_EnvstepsSoFar : 66001\n","Train_AverageReturn : -109.57667345567083\n","Train_BestReturn : -109.57667345567083\n","TimeSinceStart : 276.12909269332886\n","Training Loss : 0.28424447774887085\n","Done logging...\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1VUW3K6NDKMM"},"source":["### Visualize vanilla DQN results on Lunar Lander\n","%load_ext tensorboard\n","%tensorboard --logdir logs/dqn/LunarLander/vanilla_dqn"],"id":"1VUW3K6NDKMM","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qe2Y5_iDDKMZ"},"source":["## Double DQN\n","One potential issue with learning our Q functions with bootstrapping is _maximization bias_, where the learned Q-values tend to overestimate the actual expected future returns. The main idea is that when there is estimation error in the next state's Q-values, even if the values were correct on average, picking the action with the maximum Q-value would tend to select one where the value is overestimated. This overoptimistic value would then also get propagated via the Bellman backups to other states and actions, and can potentially slow down learning.\n","\n","Double DQN (https://arxiv.org/abs/1509.06461) proposes a simple solution to alleviate this _maximization bias_. Instead of taking the next action that maximizes the target network's Q-value, it selects the action to maximize the _current_ Q function at the next state, and then takes the target network's estimate of that action's value. \n","\n","Implement the double DQN target value in the update method in <code>critics/dqn_critic.py</code>."],"id":"qe2Y5_iDDKMZ"},{"cell_type":"code","metadata":{"id":"5YQnXPTuDKMa"},"source":["#### Test DQN target value with double Q\n","dqn_args = dict(dqn_base_args_dict)\n","\n","env_str = 'LunarLander'\n","dqn_args['env_name'] = '{}-v3'.format(env_str)\n","dqn_args['double_q'] = True\n","dqntrainer = DQN_Trainer(dqn_args)\n","dqnagent = dqntrainer.rl_trainer.agent\n","critic = dqnagent.critic\n","\n","ob_dim = critic.ob_dim\n","ac_dim = 6\n","N = 5\n","\n","np.random.seed(0)\n","obs = np.random.normal(size=(N, ob_dim))\n","acts = np.random.choice(ac_dim, size=(N,))\n","next_obs = np.random.normal(size=(N, ob_dim))\n","rewards = np.random.normal(size=N)\n","terminals = np.zeros(N)\n","terminals[0] = 1\n","\n","first_weight_before = np.array(ptu.to_numpy(next(critic.q_net.parameters())))\n","print(\"Weight before update (first row)\", first_weight_before[0])\n","\n","\n","loss = critic.update(obs, acts, next_obs, rewards, terminals)['Training Loss']\n","expected_loss = 0.93894196\n","loss_error = rel_error(loss, expected_loss)\n","print(\"Initial loss\", loss)\n","print(\"Initial Loss Error\", loss_error, \"should be on the order of 1e-6 or lower\")\n","\n","for i in range(4):\n","    loss = critic.update(obs, acts, next_obs, rewards, terminals)['Training Loss']\n","    print(loss)\n","\n","expected_loss = 0.7871182\n","loss_error = rel_error(loss, expected_loss)\n","print(\"Loss Error\", loss_error, \"should be on the order of 1e-6 or lower\")\n","\n","\n","first_weight_after = np.array(ptu.to_numpy(next(critic.q_net.parameters())))\n","print(\"Weight after update (first row)\", first_weight_after.shape)\n","# Test DQN gradient\n","print(first_weight_after[0])\n","weight_change_partial = first_weight_after[0] - first_weight_before[0]\n","print(weight_change_partial)\n","expected_weight_change = np.array([-0.0049137, -0.00500057, -0.00499138, -0.00491226, -0.00490116,  0.00489506,\n"," -0.00284088, -0.00171939,  0.00485736])\n","\n","\n","updated_weight_error = rel_error(weight_change_partial, expected_weight_change)\n","print(\"Weight Update Error\", updated_weight_error, \"should be on the order of 1e-6 or lower\")"],"id":"5YQnXPTuDKMa","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ww4kye8zDKMa"},"source":["We can now also run some experiments on LunarLander with Double DQN. You may be able to see that double DQN performs slightly better and more stably, but as there is very high variance, dont' worry if you do not."],"id":"Ww4kye8zDKMa"},{"cell_type":"code","metadata":{"id":"FlGWXjaxDKMb"},"source":["# Run with double DQN\n","dqn_args = dict(dqn_base_args_dict)\n","\n","env_str = 'LunarLander'\n","dqn_args['env_name'] = '{}-v3'.format(env_str)\n","dqn_args['double_q'] = True\n","\n","# Delete all previous logs\n","remove_folder('logs/dqn/{}/double_dqn'.format(env_str))\n","\n","for seed in range(3):\n","    print(\"Running DQN experiment with seed\", seed)\n","    dqn_args['seed'] = seed\n","    dqn_args['logdir'] = 'logs/dqn/{}/double_dqn/seed{}'.format(env_str, seed)\n","    dqntrainer = DQN_Trainer(dqn_args)\n","    dqntrainer.run_training_loop()"],"id":"FlGWXjaxDKMb","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nzthwpGVDKMc"},"source":["### Visualize all DQN results on Lunar Lander\n","%load_ext tensorboard\n","%tensorboard --logdir logs/dqn/LunarLander/"],"id":"nzthwpGVDKMc","execution_count":null,"outputs":[]}]}